# QUICK START - Colab Execution

## ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø Colab-‡¶è ‡¶è‡¶á ‡ß©‡¶ü‡¶ø command ‡¶ö‡¶æ‡¶≤‡¶æ‡¶ì (‡¶Ø‡¶•‡¶æ‡¶ï‡ßç‡¶∞‡¶Æ‡ßá):

### **Step 1: Training**
```bash
python train.py
```

**‡¶ï‡ßÄ ‡¶π‡¶¨‡ßá**: 
- Vanilla RNN, LSTM, LSTM+Attention, Transformer train ‡¶π‡¶¨‡ßá
- 15 epochs ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø model
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø epoch ‡¶è‡¶∞ training/validation curves save ‡¶π‡¶¨‡ßá
- **Checkpoints**: `/checkpoints/{model}_best.pt` ‡¶è‡¶¨‡¶Ç `_latest.pt`
- **Training time**: ~1.5-2 ‡¶ò‡¶£‡ßç‡¶ü‡¶æ

---

### **Step 2: Full Evaluation**
```bash
python evaluate_all_models.py
```

**‡¶ï‡ßÄ ‡¶π‡¶¨‡ßá**:
- ‡¶∏‡¶¨ models evaluate ‡¶π‡¶¨‡ßá test set-‡¶è
- **3‡¶ü‡¶ø comparison plots** generate ‡¶π‡¶¨‡ßá:
  - `model_comparison.png` - BLEU, Token Accuracy, Exact Match
  - `performance_vs_length.png` - Length vs performance
  - `error_analysis.png` - Error breakdown

- **JSON results**: 
  - `vanilla_rnn_evaluation.json`
  - `lstm_evaluation.json`
  - `lstm_attention_evaluation.json`
  - `evaluation_summary.json`

**Runtime**: ~5-10 ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü

---

### **Step 3: Attention Visualization** (LSTM+Attention only)
```bash
python visualize_attention_final.py
```

**‡¶ï‡ßÄ ‡¶π‡¶¨‡ßá**:
- Test set ‡¶•‡ßá‡¶ï‡ßá ‡ß©‡¶ü‡¶ø random example pick ‡¶ï‡¶∞‡¶¨‡ßá
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø example ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø:
  - Docstring input ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá
  - Reference code ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá
  - Generated code ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá
  - **Attention heatmap save ‡¶π‡¶¨‡ßá** (color intensity = attention weight)
  - Top attended words analyze ‡¶ï‡¶∞‡¶¨‡ßá

**Output files**:
```
/checkpoints/attention_visualizations/
‚îú‚îÄ‚îÄ attention_example_1.png  ‚Üê Heatmap visualization
‚îú‚îÄ‚îÄ attention_example_2.png
‚îî‚îÄ‚îÄ attention_example_3.png
```

**Runtime**: ~2-3 ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü

---

## Expected Output

### Console Output (Training):
```
Using seed: 42
Using device: cuda
Loading dataset from Hugging Face...
Train: 10000, Val: 1500, Test: 1500

============================================================
Training vanilla_rnn
============================================================
Epoch 1/15
Training vanilla_rnn: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:45<00:00, 3.45it/s]
Train Loss: 6.2341 | Val Loss: 5.8234
‚úì Best model updated!

Epoch 2/15
...
```

### Evaluation Output:
```
======================================================================
Evaluating vanilla_rnn...
======================================================================
‚úì vanilla_rnn model created successfully!
Evaluating model...

BLEU Score:              0.2345 (¬±0.1523)
Token Accuracy:          35.42%
Exact Match Accuracy:    2.34%

Error Analysis (out of 500 examples):
  Syntax Errors:         145
  Missing Indentation:   78
  Incorrect Operators:   203
======================================================================

[Comparison plot visualization]
[Performance vs length plot]
[Error analysis plot]
```

### Attention Visualization Output:
```
======================================================================
Example 1
======================================================================

Docstring (input):
  returns list of integers between min and max values

Reference (expected):
  def get_range ( min_val , max_val ) : return [ i for i in range ( ...

Generated (model output):
  def range_list ( start , end ) : return [ i for i in range ( start , ...

üìä Attention Analysis:
  Top attended source tokens:
    - 'list': 0.234
    - 'integers': 0.189
    - 'between': 0.145
  Attention entropy (lower=more focused): 2.145
  Diagonal alignment score: 0.456

[Heatmap visualization with color intensity = attention weight]
```

---

## Sample Results (Expected)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model           ‚îÇ BLEU ‚Üë   ‚îÇ Token Acc  ‚îÇ Exact Match ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Vanilla RNN     ‚îÇ 0.230    ‚îÇ 35.2%      ‚îÇ 1.5%        ‚îÇ
‚îÇ LSTM            ‚îÇ 0.320    ‚îÇ 42.1%      ‚îÇ 3.2%        ‚îÇ
‚îÇ LSTM+Attention  ‚îÇ 0.420    ‚îÇ 51.3%      ‚îÇ 6.8%        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Files Required in Checkpoints Before Step 2 & 3

After training (Step 1) complete, you should have:
```
/content/drive/MyDrive/text2code-seq2seq/checkpoints/
‚îú‚îÄ‚îÄ vanilla_rnn_best.pt           ‚Üê Required
‚îú‚îÄ‚îÄ lstm_best.pt                  ‚Üê Required
‚îú‚îÄ‚îÄ lstm_attention_best.pt        ‚Üê Required (for visualization)
‚îú‚îÄ‚îÄ docstring_vocab.pkl
‚îú‚îÄ‚îÄ code_vocab.pkl
‚îî‚îÄ‚îÄ *.png (training curves)
```

---

## Resume Training (If GPU Crashes)

Just run Step 1 again - it will automatically resume from last checkpoint:
```bash
python train.py  # Resumes from where it stopped
```

Delete checkpoints to start fresh:
```python
import os
checkpoint_dir = '/content/drive/MyDrive/text2code-seq2seq/checkpoints'
for f in os.listdir(checkpoint_dir):
    if '_latest.pt' in f or '_best.pt' in f:
        os.remove(os.path.join(checkpoint_dir, f))
```

---

## Troubleshooting

**Out of Memory:**
- Reduce batch_size in train.py: `"batch_size": 32` (from 64)

**Slow Evaluation:**
- In evaluate_all_models.py, reduce examples: `max_examples=100`

**NLTK Error:**
```python
import nltk
nltk.download('averaged_perceptron_tagger')
```

**Drive Not Mounted:**
```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
```

---

## All Requirements Covered ‚úÖ

- ‚úÖ Token-level Accuracy
- ‚úÖ BLEU Score
- ‚úÖ Exact Match Accuracy
- ‚úÖ Syntax Error Detection
- ‚úÖ Indentation Error Detection
- ‚úÖ Operator Error Detection
- ‚úÖ Performance vs Length Analysis
- ‚úÖ Attention Visualization (3+ examples)
- ‚úÖ Model Comparison
- ‚úÖ Reproducibility (seed=42)
- ‚úÖ Extended Lengths (100/150 tokens)
- ‚úÖ Checkpoint Resume

**All done! Just run the 3 commands in order.** üéâ
